# 🧪 테스트 가이드

## 최적화 후 테스트 방법

---

## 1. 백엔드 서버 재시작

```powershell
cd "c:\Users\helen\Desktop\kt cloud tech up\basic_project\web_all\web_back-main"

# 기존 서버 종료 (실행 중이라면)
# Ctrl+C로 종료

# 서버 재시작
python multi_main.py
```

---

## 2. 테스트 시나리오

### ✅ 시나리오 1: 개별 해석 품질 확인

**목표**: 캡션에서 구체적 특징을 추출하고 심리적 의미를 정확히 해석하는지 확인

**테스트 방법**:
1. 각 그림(집, 나무, 사람)을 그리기
2. 개별 해석 섹션 확인
3. 다음 사항을 체크:
   - ✓ 캡션의 구체적 요소 언급 (예: "큰 나무", "작은 창문")
   - ✓ 각 요소의 심리적 의미 설명
   - ✓ 통합된 종합 분석
   - ✓ 중간에 끊기지 않고 완전한 해석
   - ✓ 반복되는 문장이 없음

**좋은 출력 예시**:
```
Feature Analysis:
- Large tree with prominent trunk: Suggests strong ego strength...
- Absence of roots: May indicate feeling ungrounded...

Psychological Synthesis:
The drawing overall suggests an individual with...
```

---

### ✅ 시나리오 2: 질문 생성 품질 확인

**목표**: 그림의 구체적 요소에 대한 명확한 질문 생성 여부 확인

**테스트 방법**:
1. 해석 완료 후 채팅 창 확인
2. 5개 질문이 자동으로 생성되는지 확인
3. 각 질문이 다음 조건을 만족하는지 체크:
   - ✓ 한 문장으로 명확하게 끝남 (물음표로 종료)
   - ✓ 그림의 구체적 요소 언급 (집의 창문, 나무의 뿌리, 사람의 손 등)
   - ✓ 왜 특정 방식으로 그렸는지 질문
   - ✓ 메타 질문이 아님 ("계속할까요?" 같은 것 없음)

**좋은 질문 예시**:
```
- "Why did you draw the house without a chimney?"
- "Can you tell me about your choice to place the tree on the left side?"
- "What made you decide not to include hands in the person drawing?"
```

**나쁜 질문 예시** (이런 게 나오면 안됨):
```
- "Shall I continue with more questions?"
- "Do you want me to provide more context?"
- "Can you provide additional information?"
```

---

### ✅ 시나리오 3: RAG 통합 확인

**목표**: RAG 문서가 적절히 참조되고 있는지 확인

**테스트 방법**:
1. 백엔드 로그 확인 (`multi_main.py` 실행 터미널)
2. `[RAG] 검색 완료` 로그에서:
   - 검색된 문서 수 확인
   - 문서 내용 미리보기 확인
3. `[INTERPRET_SINGLE] 해석 완료` 로그에서:
   - "RAG 문서를 참고하여 해석" 메시지 확인
4. 개별 해석이 RAG 문서의 전문 지식을 반영하는지 확인

---

### ✅ 시나리오 4: 최종 해석 통합

**목표**: 3개 그림 + 대화 내용이 잘 통합되는지 확인

**테스트 방법**:
1. 5개 질문에 모두 답변
2. "최종 종합 해석" 섹션 확인
3. 다음 사항 체크:
   - ✓ 집, 나무, 사람 해석이 모두 언급됨
   - ✓ 대화 내용이 반영됨
   - ✓ 5개 문단으로 구성됨
   - ✓ 전문적이고 따뜻한 어조
   - ✓ 한국어로 자연스럽게 작성됨

---

## 3. 문제 발생 시 체크리스트

### 🔴 해석이 이상하게 나올 때

1. **로그 확인**:
   ```
   [PROMPT] 해석 생성 프롬프트:
   ```
   - 프롬프트가 올바른 형식인지 확인
   - 캡션이 정확히 들어갔는지 확인

2. **출력 확인**:
   ```
   Generated XXX characters
   ```
   - 글자 수가 너무 적으면 (<100) 문제 있음
   - 너무 많으면 (>1500) 반복 가능성

3. **모델 파라미터 조정 고려**:
   `model.py`에서:
   ```python
   temperature=0.65  # 더 낮추면 (0.5) 더 보수적
   repetition_penalty=1.1  # 높이면 (1.2) 반복 더 줄어듦
   ```

---

### 🔴 질문이 이상할 때

1. **후처리 로직 확인**:
   ```
   [QUESTIONS] 최종 질문: ...
   ```
   - 물음표로 끝나는지 확인
   - 한 문장인지 확인

2. **대화 컨텍스트 길이**:
   - 해석이 너무 길면 요약됨 (각 200자)
   - 필요시 `multi_main.py`의 요약 길이 조정

---

### 🔴 서버 오류

1. **모델 로딩 실패**:
   ```bash
   # 로그에서 확인
   Loading Qwen HTP Model: helena29/Qwen2.5_LoRA_for_HTP
   ```
   - 네트워크 문제면 캐시 사용됨
   - GPU 메모리 부족이면 CPU로 전환됨

2. **토큰 제한 초과**:
   - RAG 문서가 너무 많으면 발생
   - `multi_main.py`에서 문서 수 제한 확인 (최대 3개)

---

## 4. 성능 비교

### 📊 최적화 전 vs 후 비교표

| 항목 | 최적화 전 | 최적화 후 |
|------|----------|----------|
| **개별 해석 완성도** | 중간에 끊김 가능 | 완전한 해석 |
| **질문 명확성** | 가끔 모호한 질문 | 구체적인 질문 |
| **반복 문제** | 간혹 반복 발생 | 반복 최소화 |
| **RAG 통합** | 문서 과다 로드 | 적절한 요약 |
| **응답 시간** | 유사 | 유사 |
| **일관성** | 보통 | 향상됨 |

---

## 5. 추가 모니터링

### 📈 로그에서 확인할 주요 지표

1. **생성 토큰 수**:
   ```
   Generated XXX characters
   ```
   - 해석: 300-800자가 이상적
   - 질문: 50-150자가 이상적

2. **RAG 검색 결과**:
   ```
   검색된 문서 수: X
   ```
   - 1-3개가 이상적
   - 0개면 일반 지식으로 해석
   - 5개 이상이면 너무 많음

3. **프롬프트 길이**:
   ```
   프롬프트 길이: XXXX characters
   ```
   - 500-1500자가 적절
   - 2000자 이상이면 너무 김

---

## 6. 성공 기준

✅ **최소 기준**:
- 해석이 캡션 내용을 언급함
- 질문이 그림 요소와 관련됨
- 중간에 끊기지 않음
- 명백한 오류 없음

🎯 **목표 기준**:
- 캡션의 모든 주요 요소 분석
- 각 요소의 심리적 의미 설명
- 구체적이고 통찰력 있는 질문
- 전문적이고 일관된 어조
- 사용자 대화 내용 반영

---

## 🆘 문제 해결

문제가 계속되면:
1. 로그 전체 복사
2. 입력 캡션 확인
3. 모델 출력 확인
4. 위 정보 공유

원본 복구:
```powershell
Copy-Item "multi_main_backup.py" "multi_main.py" -Force
```

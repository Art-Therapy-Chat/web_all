from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í•œ ë²ˆë§Œ ë¡œë“œ
_model = None
_tokenizer = None
_model_name = "helena29/Qwen2.5_LoRA_for_HTP"

def _load_model():
    """ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _model, _tokenizer
    
    if _model is None:
        print(f"ğŸ”¥ Loading Qwen HTP Model: {_model_name}")
        print(f"ğŸ” CUDA Available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ğŸ” CUDA Device: {torch.cuda.get_device_name(0)}")
            print(f"ğŸ” CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        _tokenizer = AutoTokenizer.from_pretrained(_model_name)
        
        # ëª¨ë¸ ë¡œë“œ (LoRA ì–´ëŒ‘í„°ê°€ ì´ë¯¸ ë³‘í•©ëœ ìƒíƒœ)
        _model = AutoModelForCausalLM.from_pretrained(
            _model_name,
            device_map="auto",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        print(f"âœ… Qwen HTP Model loaded successfully!")
        print(f"âœ… Model Device: {_model.device}")
    
    return _model, _tokenizer


def _clean_output(text: str) -> str:
    """
    ëª¨ë¸ ì¶œë ¥ í›„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±° ë° ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì²˜ë¦¬
    """
    import re
    
    # ë”°ì˜´í‘œë‚˜ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
    text = text.strip('`"\'').strip()
    
    # "Output:", "Answer:", "Response:" ê°™ì€ í”„ë¦¬í”½ìŠ¤ ì œê±°
    text = re.sub(r'^(Output|Answer|Response|Result):\s*', '', text, flags=re.IGNORECASE)
    
    # ì—°ì†ëœ ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆ ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r' {2,}', ' ', text)
    
    # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ê°ì§€ ë° ì œê±°
    text = text.strip()
    if text and text[-1] not in '.!?ã€‚':
        # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ ë¶€í˜¸ ì°¾ê¸°
        last_complete_idx = -1
        for i in range(len(text) - 1, -1, -1):
            if text[i] in '.!?ã€‚':
                last_complete_idx = i
                break
        
        # ì™„ì „í•œ ë¬¸ì¥ì´ ìˆìœ¼ë©´ ê±°ê¸°ê¹Œì§€ë§Œ ìœ ì§€
        if last_complete_idx > 0:
            text = text[:last_complete_idx + 1]
    
    return text.strip()


def generate_with_qwen(prompt: str):
    """
    Qwen ëª¨ë¸ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ìƒì„±
    ëª¨ë¸ì€ ìµœì´ˆ 1íšŒë§Œ ë¡œë“œë˜ê³  ì¬ì‚¬ìš©ë¨
    """
    # ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©)
    model, tokenizer = _load_model()
    
    print("=" * 80)
    print("ğŸ“ [PROMPT] í•´ì„ ìƒì„± í”„ë¡¬í”„íŠ¸:")
    print("-" * 80)
    print(prompt)
    print("=" * 80)
    
    print(f"ğŸ” [generate_with_qwen] Model device: {model.device}")
    
    # ì…ë ¥ í…ì„œ ì¤€ë¹„
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # ëª¨ë“  ì…ë ¥ì„ ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    print(f"ğŸ” [generate_with_qwen] Input device: {inputs['input_ids'].device}")
    
    # ìƒì„± - fine-tuned ëª¨ë¸ì— ìµœì í™”ëœ íŒŒë¼ë¯¸í„°
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=500,   # ì ì ˆí•œ ê¸¸ì´ë¡œ ì¡°ì •
            min_new_tokens=150,   # ìµœì†Œ ê¸¸ì´ ë³´ì¥
            temperature=0.3,     # ì•½ê°„ ë‚®ì¶°ì„œ ì¼ê´€ì„± í–¥ìƒ
            top_p=0.9,            # nucleus sampling ì¶”ê°€
            do_sample=True,
            repetition_penalty=1.15,  # ë°˜ë³µ ë°©ì§€ ê°•í™”
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            no_repeat_ngram_size=3  # 3-gram ë°˜ë³µ ë°©ì§€
        )
    
    # í”„ë¡¬í”„íŠ¸ ì œê±°: ì…ë ¥ í† í° ì´í›„ë§Œ ì¶”ì¶œ
    input_len = inputs["input_ids"].shape[1]
    generated_ids = outputs[0][input_len:]
    
    result = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    
    # ì¶œë ¥ í›„ì²˜ë¦¬
    result = _clean_output(result)
    
    print(f"âœ… [generate_with_qwen] Generated {len(result)} characters")
    
    return result
